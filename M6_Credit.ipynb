{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a5cc8190",
      "metadata": {
        "id": "a5cc8190"
      },
      "source": [
        "# Module 6: CFA\n",
        "# Use Case: Credit Risk - Identifying Bad Credit Risks\n",
        "# ===========================================\n",
        "\n",
        "In this example, we aim to predict bad consumer credits, and we develop a classification model for this purpose, driven by loan and debtor attributes. We would use this model to accept or reject a customerâ€™s business.\n",
        "\n",
        "Dataset: Credit risk https://datahub.io/machine-learning/credit-g\n",
        "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "id": "35f08271",
      "metadata": {
        "id": "35f08271"
      },
      "outputs": [],
      "source": [
        "# package for working with tabular data\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "\n",
        "# Package for charting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns #charts\n",
        "\n",
        "# package for timing runtime\n",
        "import time\n",
        "\n",
        "# package for navigating the operating system\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities: Governance and Fairness\n",
        "\n",
        "First we will declare and introduce the functions we will be using to implement the Governance Framework, and principles of Fairness in our process.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0xu8WyTTVNGd"
      },
      "id": "0xu8WyTTVNGd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Governance Framework and Utility Function Overview\n",
        "\n",
        "A Governance framework ensures models in an organisation achieve all their key stakeholder requirements satisfactorily, and in a safe, verifiable way. In this section we introduce functions we will use lter in thje use-case to control and monitor model development process to ensure our aims are achieved.\n",
        "\n",
        "Manys steps in a Governance Framework are qualitative, requiring professionals to assess, specify, approve or reject stages in model development. However, quantitative tools can be a powerful utility, allowing professionals to control and monitor a process, and reach judgements about model design, stability, and efficacy. \n",
        "\n",
        "We discuss the 5 stages of model development and the utility functions that can be used to support the Governance Framework:\n",
        "\n",
        "##### Stage1: Business Analysis\n",
        "We first define our stakeholder KPIs, which should be systematically defined. We introduce example functions that go some way to representing stakeholder KPIs, with visualizations, statistical tests and checks where appropriate. \n",
        "\n",
        "##### Stage2: Data Process\n",
        "Exploratory data analysis goes some way to examining the quality and nature of the data, looking at distributions, correlations, imbalances in the data. We use some utility functions to support this.\n",
        "\n",
        "##### Stage3: Model Design and Development\n",
        "From a governance point of view, model design and development is more qualitative, and requires good practice, statitically and in terms of the code implementation. Good commenting is essential, sanity checking of input and return values is advised, and in Python clear parameter declaration and control of source code, and code versions is essential. \n",
        "\n",
        "##### Stage4: Model Deployment\n",
        "Model deployment involes multie stages of testing and authorization. We propose a challenger model to conduct part of this process, which is also used in the monitoring and reporting stage also.\n",
        "\n",
        "##### Stage5: Monitoring/Reporting\n",
        "During live running of the models, monitoring of data drift is essential, and for additional safety a challenger model can be run in parallel to the live model, to ensure the live model is functioning well with respect to stakeholder KPIs.\n"
      ],
      "metadata": {
        "id": "JE0VW_oeVnah"
      },
      "id": "JE0VW_oeVnah"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage1 Business Analysts Governance Utilities\n",
        "\n",
        "#### Stakeholder KPIs\n",
        "In Stage1, Business Analysis, we will find that we have several stakeholders for the model. We need analytics and visualizations to verify that we are attaining good performance on each stakeholder's key performance indicators (KPI) We list the stakeholder governance functions we will use as utilities in this case study now..."
      ],
      "metadata": {
        "id": "wbY4WfepSk9J"
      },
      "id": "wbY4WfepSk9J"
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC Curve to visualize classifier accuracy.\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "# Performance metrics...\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# Analyst KPI: ROC Curve, f1, precision and accuracy of y_hat from a classifier\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "def kpi_review_analyst(mdl: object,\n",
        "                       X: np.array, \n",
        "                       y: np.array,\n",
        "                       y_hat: np.array) -> (float, float, float):\n",
        "   \n",
        "  '''\n",
        "  Args:\n",
        "       mdl: sklearn classifier model object\n",
        "       X: X variables, columns are features, rows are instances\n",
        "       y: actual target variable {1,0}\n",
        "       y_hat: prediction of target\n",
        "       \n",
        "    Returns:\n",
        "       f1, prec, rec: F1 score, precision, recall.\n",
        "       \n",
        "    Author:\n",
        "       Dan Philps\n",
        "    '''\n",
        "\n",
        "  #sanity\n",
        "  if X.shape[0] != y_test.shape[0]:\n",
        "    raise TypeError('Bad parameter: X.shape[0] != y_test.shape[0]')\n",
        "  if y.shape[0] != y_hat.shape[0]:\n",
        "    raise TypeError('Bad parameter: y_test.shape[0] != y_test_hat.shape[0]')\n",
        "  if (y.dtype != y_hat.dtype):\n",
        "    raise TypeError('Bad parameter: y_test.dtypes != y_test_hat.dtypes')\n",
        "\n",
        "  # F1, precision, recall...  \n",
        "  prec = precision_score(y_true=y[:], y_pred=y_hat[:])\n",
        "  rec = recall_score(y_true=y[:], y_pred=y_hat[:])\n",
        "  f1 = f1_score(y_true=y[:], y_pred=y_hat[:])\n",
        "\n",
        "  print(prec)\n",
        "\n",
        "  # ROC Curve\n",
        "  metrics.plot_roc_curve(mdl, X, y) \n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y, y_hat)\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
        "                                    estimator_name='example estimator')\n",
        "  display.plot()\n",
        "  plt.show()\n",
        "\n",
        "  return f1, prec, rec"
      ],
      "metadata": {
        "id": "2rm-orUwNZ8W"
      },
      "id": "2rm-orUwNZ8W",
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 2 Data Process Governance Utilities\n",
        "\n",
        "Exploratory data analysis goes some way to examining the quality and nature of the data, looking at distributions, correlations, imbalances in the data. "
      ],
      "metadata": {
        "id": "SzlVEOCtH9cR"
      },
      "id": "SzlVEOCtH9cR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation between a model's features can yield unreliable results. For linear models, like linear regression or \n",
        "# logistic regression, multicolinearity can yield solutions that are wildly varying and possibly unstable. Although \n",
        "# random forests can be go\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "\n",
        "def correlation_between_features(X: pd.DataFrame):\n",
        "    '''\n",
        "    Args:\n",
        "        dataframe_ex_target: Dataframe of the features, without the target column\n",
        "       \n",
        "    Returns:\n",
        "       correlation_matrix\n",
        "\n",
        "    Author: \n",
        "      Augustine Backer\n",
        "    '''\n",
        "    correlation_matrix = X.corr().abs()\n",
        "    print(correlation_matrix)\n",
        "    sns.heatmap(correlation_matrix, annot=True)\n",
        "    plt.show()\n",
        "\n",
        "    return correlation_matrix"
      ],
      "metadata": {
        "id": "RFoXKfeUICBw"
      },
      "id": "RFoXKfeUICBw",
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalances also need to be checked in the y variable. We can simple plot a pie chart to examne this. "
      ],
      "metadata": {
        "id": "wAHTGQAMRWr7"
      },
      "id": "wAHTGQAMRWr7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for imbalances, charts pie of imbalances in the y variable \n",
        "# wrapped as a func as we will use it a few times..\n",
        "def imbalanced_y_check(y: pd.Series, \n",
        "                  class_col: str = 'class') -> bool:  \n",
        "  '''\n",
        "    Args:\n",
        "        y: Dataframe of only the y variable\n",
        "       \n",
        "    Returns:\n",
        "       bool: True if imbalanced, False, if not imbalanced.\n",
        "\n",
        "    Author: \n",
        "      Dan Philps\n",
        "    '''\n",
        "  \n",
        "  if type(y) == \"pandas.core.series.Series\":\n",
        "    y =  pd.DataFrame(y)\n",
        "\n",
        "  # sanity  \n",
        "  if (class_col in y.columns):\n",
        "    raise TypeError('Series column name is not ' + class_col)\n",
        "\n",
        "  print('Dataset Balanced?')\n",
        "  print(y[class_col].value_counts())\n",
        "  y.groupby(class_col).size().plot(kind='pie', y=class_col, label = \"Type\",  autopct='%1.1f%%')\n",
        "\n",
        "  #Rule of thumb... highest frequency class < 70% of observations\n",
        "  imbalanced = False\n",
        "  perc_split = y[class_col].value_counts() / y.shape[0]\n",
        "  if np.max(perc_split) >= 0.7:\n",
        "    print('Imbalanced y variable!')\n",
        "    imbalanced = True\n",
        "  \n",
        "  return imbalanced\n",
        "\n",
        "imbalanced_y_check(df['class'])\n",
        "type(df['class'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "WnD3Vn0yRW9t",
        "outputId": "3bf7b232-c863-48b8-893c-ec052f58fc21"
      },
      "id": "WnD3Vn0yRW9t",
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-221-079f303da231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mimbalanced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mimbalanced_y_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-221-079f303da231>\u001b[0m in \u001b[0;36mimbalanced_y_check\u001b[0;34m(y, class_col)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# sanity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclass_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Series column name is not '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'columns'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(perc_split)"
      ],
      "metadata": {
        "id": "gu51S3qudwN_",
        "outputId": "56d80dbc-81cc-450c-979a-18a74a856e47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gu51S3qudwN_",
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 3 Model Design and Development Governance Utilities"
      ],
      "metadata": {
        "id": "FwJLI-GCIVTC"
      },
      "id": "FwJLI-GCIVTC"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IjrF9ej3IVir"
      },
      "id": "IjrF9ej3IVir",
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 4 Model Deployment Governance Utilities"
      ],
      "metadata": {
        "id": "CKA0l02DWtyo"
      },
      "id": "CKA0l02DWtyo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Challenger Models\n",
        "\n",
        "The following function, trains a challenger model using sklearn classifiers, and can be used to challenge our live model by comparing KPIs between our live  and challenger models.\n"
      ],
      "metadata": {
        "id": "GRd3jH02K_gN"
      },
      "id": "GRd3jH02K_gN"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Create a challenger model by \n",
        "def challenger_ensemble_run(all_mdls: list, \n",
        "                  all_mdls_desc: list,\n",
        "                  all_mdls_f1score: list,\n",
        "                  X_train: pd.DataFrame,\n",
        "                  y_train: pd.DataFrame, \n",
        "                  X_test: pd.DataFrame) -> (np.array, object):\n",
        "  '''\n",
        "  Args:\n",
        "    all_mdls: a list of sklearn classifiers, trained and ready to go.\n",
        "    all_mdls_desc: list of description of the above model objects (elements corresponding to the above also)       \n",
        "    all_mdls_f1score: list of out-of-sample F1 scores for each of the above model objects, used to elmininate the poor performers from the challenger (elements corresponding to the above also)       \n",
        "    X_train: DataFrame with training data for classifier, columns are features, rows are instances\n",
        "    X_test: Test data matching above shape\n",
        "    y_train: training data target variable {1,0}, instances are rows.\n",
        "    y_test: test data target variable {1,0}, instances are rows.\n",
        "      \n",
        "  Returns:\n",
        "      y_hat: numpy array containing predictions from the challenger\n",
        "      object: sklearn model object containing the challenger model.\n",
        "      \n",
        "  Author:\n",
        "      Dan Philps\n",
        "  '''\n",
        "\n",
        "  #Sanity\n",
        "  if X_train.shape[0] != y_train.shape[0]:\n",
        "      raise TypeError('Bad parameter: X_train.shape[0] != y_train.shape[0]')\n",
        "  if X_test.shape[0] != X_train.shape[0]:\n",
        "    raise TypeError('Bad parameter: X_train.shape[0] != y_train.shape[0]')\n",
        "\n",
        "  # Only use classifiers that have generated an above median F1 score out of sample.\n",
        "  min_f1_to_use = np.median(all_mdls_f1score)\n",
        "  \n",
        "  # Prepare our models for ensembling\n",
        "  challenger_models = []\n",
        "  for i in range(0, all_mdls_desc.__len__()):\n",
        "    if all_mdls_f1score[i] > min_f1_to_use:\n",
        "      challenger_models.append((all_mdls_desc[i], all_mdls[i]))\n",
        "\n",
        "  # Instantiate ...\n",
        "  vc = VotingClassifier(estimators=challenger_models, voting='soft')\n",
        "\n",
        "  # Fit on the training data to train your live model... \n",
        "  challenger_mdl = vc.fit(X_train, y_train)\n",
        "\n",
        "  # Challenge! Compare the results\n",
        "  y_hat = challenger_mdl.predict(X_test)\n",
        "  \n",
        "  return y_hat, object\n"
      ],
      "metadata": {
        "id": "wpRpI2MQK_zr"
      },
      "id": "wpRpI2MQK_zr",
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage5 Governance Utilities\n",
        "\n",
        "#### Data drift monitoring\n",
        "After our model is released into the live environment, we need to ensure that the input data is not significantly different to tghe data the model was trained on. If it is the model outcomes could become unstable. We can use population stability index (PSI) to compare input data with training data, and we can also use a KS test to do the same. "
      ],
      "metadata": {
        "id": "1O4k1SyqEt9_"
      },
      "id": "1O4k1SyqEt9_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Population Stability Index (PSI) can be applied to the input features or variables, also known as Characteristic \n",
        "# Stability Index (CSI), as well as the output of a scoring model, a model whose score may indicate the probability \n",
        "# of fraud, or probability of default. PSI captures the shift in the population distribution of values. If the score\n",
        "# distribution has shifted, one should then look to see what feature(s) or variable(s) is causing the shift.\n",
        "# A shift in the distribution of input features or features, or output score distribution could imply that the model \n",
        "# may need retrained. The common interpretation of PSI, which comes from the orignal work on credit models, is as follows: \n",
        "# PSI < 0.1: no significant population change, PSI < 0.2: moderate population change and PSI >= 0.2: significant population\n",
        "# change.\n",
        "\n",
        "#https://www.quora.com/What-is-population-stability-index\n",
        "\n",
        "def calculate_psi(X_train: np.array,                  \n",
        "                  X: np.array, \n",
        "                  buckettype: str='bins', \n",
        "                  buckets: int =10, \n",
        "                  axis: int =0) -> np.ndarray:\n",
        "    '''Calculate the PSI (population stability index) across all variables\n",
        "    Args:\n",
        "       expected: numpy matrix of original values - both features and target\n",
        "       actual: numpy matrix of new values, same size as expected\n",
        "       buckettype: type of strategy for creating buckets, bins splits into even splits, \n",
        "       quantiles splits into quantile buckets\n",
        "       buckets: number of quantiles to use in bucketing variables\n",
        "       axis: axis by which variables are defined, 0 for vertical, 1 for horizontal \n",
        "       \n",
        "    Returns:\n",
        "       psi_values: ndarray of psi values for each variable\n",
        "       \n",
        "    Author:\n",
        "       Matthew Burke\n",
        "       github.com/mwburke\n",
        "       worksofchart.com\n",
        "    '''\n",
        "\n",
        "    # Sanity\n",
        "    if X_train.shape[1] != X.shape[1]:\n",
        "      raise TypeError('X_train.shape[1] != X.shape[1]')\n",
        "\n",
        "    def psi(expected_array: np.array,        \n",
        "            actual_array: np.array,        \n",
        "            buckets: int) -> np.ndarray:\n",
        "        '''Calculate the PSI for a single variable \n",
        "        Args:\n",
        "           expected_array: numpy array of original values\n",
        "           actual_array: numpy array of new values, same size as expected\n",
        "           buckets: number of percentile ranges to bucket the values into\n",
        "           \n",
        "        Returns:\n",
        "           psi_value: calculated PSI value\n",
        "        \n",
        "        Author: Augustine Backer\n",
        "        '''\n",
        "        \n",
        "        def scale_range (input,\n",
        "                         min, \n",
        "                         max):\n",
        "            input += -(np.min(input))\n",
        "            input /= np.max(input) / (max - min)\n",
        "            input += min\n",
        "            return input\n",
        "\n",
        "        breakpoints = np.arange(0, buckets + 1) / (buckets) * 100\n",
        "        \n",
        "        if buckettype == 'bins':\n",
        "            breakpoints = scale_range(breakpoints, np.min(expected_array), np.max(expected_array))\n",
        "        elif buckettype == 'quantiles':\n",
        "            breakpoints = np.stack([np.percentile(expected_array, b) for b in breakpoints])\n",
        "    \n",
        "        expected_percents = np.histogram(expected_array, breakpoints)[0] / len(expected_array)\n",
        "        actual_percents = np.histogram(actual_array, breakpoints)[0] / len(actual_array)\n",
        "    \n",
        "\n",
        "        def sub_psi(e_perc: float, \n",
        "                    a_perc: float) -> float:\n",
        "            '''Calculate the actual PSI value from comparing the values.\n",
        "               Update the actual value to a very small number if equal to zero\n",
        "            '''\n",
        "            if a_perc == 0:\n",
        "                a_perc = 0.001\n",
        "\n",
        "                a_perc = 0.0001\n",
        "            if e_perc == 0:\n",
        "                e_perc = 0.0001\n",
        "\n",
        "            value = (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
        "            return(value)\n",
        "        \n",
        "        psi_value = sum(sub_psi(expected_percents[i], actual_percents[i]) for i in range(0, len(expected_percents)))\n",
        "\n",
        "        return(psi_value)\n",
        "    \n",
        "    # The shape function returns the dimension of the array, with 1 being one variable being examined\n",
        "    # psi_values - a psi value will be calculated for each variable in the array\n",
        "    if len(X_train.shape) == 1:\n",
        "        psi_values = np.empty(len(X_train.shape))\n",
        "    else:\n",
        "        psi_values = np.empty(X_train.shape[axis])\n",
        "    \n",
        "    for i in range(0, len(psi_values)):\n",
        "        if len(psi_values) == 1:\n",
        "            psi_values = psi(X_train, X, buckets)\n",
        "        elif axis == 0:\n",
        "            psi_values[i] = psi(X_train[:,i], X[:,i], buckets)\n",
        "        elif axis == 1:\n",
        "            psi_values[i] = psi(X_train[i,:], X[i,:], buckets)\n",
        "    \n",
        "    return(psi_values)"
      ],
      "metadata": {
        "id": "tRJUaGgnEq8X"
      },
      "id": "tRJUaGgnEq8X",
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kolmogorov-Smirnov (KS) is used to measure the performance of classification models. More accurately, KS is a measure of \n",
        "# the degree of separation between the positive and negative distributions, for example deafult vs. non-default. K-S ranges\n",
        "# from 0% to 100%, and the higher the KS value is, the better the model is at separating the positive and negative \n",
        "# distributions. The KS statistic for two samples is simply the greatest distance between their two cummulative \n",
        "# distribution functions, so if we measure the distance between the positive and negative class distributions. \n",
        "# A KS of 0.6 or higher is considered good, associated with a low p-value.\n",
        "# https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test https://towardsdatascience.com/evaluating-classification-models-with-kolmogorov-smirnov-ks-test-e211025f5573\n",
        "\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "def ks_test_result(model_score_target: pd.DataFrame,\n",
        "                   score, \n",
        "                   target) -> float:\n",
        "    '''\n",
        "    Args:\n",
        "        model_score_target: dataframe of the model output and binary target (1=event, 0=non event), with the colunns \n",
        "        labeled to match the target_column and score_column strings passed in\n",
        "        target: string of target column in the dataframe, example \"y\"\n",
        "        score: string of the score, or model output, typically [0,100], where the higher the score the higher\n",
        "        the likelihood of an event, for example \"score\"\n",
        "       \n",
        "    Returns:\n",
        "       ks_statistic: the KS value for the cummulative distribution function of the scores of events and non-events\n",
        "    \n",
        "    Author:\n",
        "      Augustine Backer\n",
        "    '''\n",
        "    ks=ks_2samp(model_score_target.loc[model_score_target[target]==0,score], \n",
        "                model_score_target.loc[model_score_target[target]==1,score])\n",
        "    \n",
        "    print(f\"KS: {ks.statistic:.4f} (p-value: {ks.pvalue:.3e})\")\n",
        "\n",
        "    return ks.statistic\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "# By itself, Kolmogorov-Smirnov (KS) is a good measure of the degree of separation between the positive and negative \n",
        "# distributions that the model can achieve, for example deafult vs. non-default. However, it is important to monitor any \n",
        "# changes in KS from the baseline period and current monitoring period. This is typically done by looking at the relative \n",
        "# percentage change. For example if KS in the baseline is 80%, or 0.80, and in the current period it is 0.60, then \n",
        "# (0.60 - 0.80)/).80 = -25%. The inner and outer thresholds for this Key Performance Indicator would be -15% to -25%.\n",
        "\n",
        "def ks_test_result_change(model_score_target_baseline, model_score_target_current, score, target):\n",
        "    '''\n",
        "    Args:\n",
        "        model_score_target_baseline: dataframe of the model output during baseline and binary target (1=event, 0=non event), with the colunns \n",
        "        labeled to match the target_column and score_column strings passed in\n",
        "        model_score_target_current: datafrmae of the model output during current monitoring period\n",
        "        target: string of target column in the dataframe, example \"y\"\n",
        "        score: string of the score, or model output, typically [0,100], where the higher the score the higher\n",
        "        the likelihood of an event, for example \"score\"\n",
        "       \n",
        "    Returns:\n",
        "       ks_relative_percent_change: the KS value for the cummulative distribution function of the scores of events and non-events\n",
        "    '''\n",
        "    ks_baseline=ks_2samp(model_score_target_baseline.loc[model_score_target_baseline[target]==0,score], \n",
        "                model_score_target_baseline.loc[model_score_target_baseline[target]==1,score])\n",
        "    ks_current=ks_2samp(model_score_target_current.loc[model_score_target_current[target]==0,score], \n",
        "                model_score_target_current.loc[model_score_target_current[target]==1,score])\n",
        "    \n",
        "    print(f\"KS Baseline: {ks_baseline.statistic:.4f} (p-value: {ks_baseline.pvalue:.3e})\")\n",
        "    print(f\"KS Monitoring Period: {ks_current.statistic:.4f} (p-value: {ks_current.pvalue:.3e})\")\n",
        "    \n",
        "    ks_relative_percent_change = (ks_current.statistic - ks_baseline.statistic)/ks_baseline.statistic\n",
        "\n",
        "    return ks_relative_percent_change"
      ],
      "metadata": {
        "id": "djgWAvcvHdpR"
      },
      "id": "djgWAvcvHdpR",
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fairness Functions\n",
        "\n",
        "@@@@@@@@@@@@@@ MADHU! DECLARE ALL YOUR FUNCTIONS HERE ... WE CAN LATER DECIDE IF WE PACKAGE UP"
      ],
      "metadata": {
        "id": "OmxxP7U6Vy3v"
      },
      "id": "OmxxP7U6Vy3v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage1. Business Case \n",
        "\n",
        "Define key performance indicators (KPIs). \n",
        "Data needed, outcomes, model risk level, communication strategy.\n",
        "Regulatory and Compliance issues \n",
        "\n",
        "Limitations of the models\n",
        "\n",
        "Assumptions made by the models\n",
        "\n",
        "Restrictions of the models...\n",
        "\n",
        "@@@@@@@@@@@@@@@@@@@@@ MADHU ARE YOU OK DEFINING KPIs HERE? 1) ACCURACY? 2) FAIRNESS\n"
      ],
      "metadata": {
        "id": "rDXnNSIt5GUs"
      },
      "id": "rDXnNSIt5GUs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage2. Data\n",
        "\n",
        "Next we load the data we need to achieve our business aims, wrangle it and prepare it."
      ],
      "metadata": {
        "id": "_DfUFfsmFWh5"
      },
      "id": "_DfUFfsmFWh5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stage2a. Load Data"
      ],
      "metadata": {
        "id": "z_M1xrqYGCJ9"
      },
      "id": "z_M1xrqYGCJ9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this is running in Google Colab.... we extract the GitHub loc where the data resides...."
      ],
      "metadata": {
        "id": "sFBrITKU_6A0"
      },
      "id": "sFBrITKU_6A0"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danphilps/credit_use_case"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CysAAt4-AdN8",
        "outputId": "ad7c94dd-0ca7-4fee-cdc1-749cce150c79"
      },
      "id": "CysAAt4-AdN8",
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'credit_use_case'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Counting objects: 100% (126/126), done.\u001b[K\n",
            "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
            "remote: Total 126 (delta 54), reused 20 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (126/126), 19.16 MiB | 8.92 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We should see a \"credit_use_case\" directory....\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LusHe3a2BF_t",
        "outputId": "a82cb23a-e5c6-4a77-9035-790148df1248"
      },
      "id": "LusHe3a2BF_t",
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['credit-g.csv',\n",
              " 'credit_use_case',\n",
              " '.git',\n",
              " 'README.md',\n",
              " '.gitattributes',\n",
              " 'Bkup',\n",
              " 'M6_Credit_BAK.ipynb',\n",
              " 'M6_Credit',\n",
              " 'M6_Credit_BAK2.ipynb',\n",
              " 'M6_Credit.ipynb',\n",
              " 'Ch20_AutoML.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('credit_use_case')"
      ],
      "metadata": {
        "id": "hhMDMb1xB0yk"
      },
      "id": "hhMDMb1xB0yk",
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "id": "c26b1a44",
      "metadata": {
        "id": "c26b1a44"
      },
      "outputs": [],
      "source": [
        "loc = \"credit-g.csv\"\n",
        "df_raw = pd.read_csv(loc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwvlymBMFqsW",
        "outputId": "cf01ae3b-6ff4-45f0-d2a9-f227ee9cf935"
      },
      "id": "pwvlymBMFqsW",
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['checking_status', 'duration', 'credit_history', 'purpose',\n",
              "       'credit_amount', 'savings_status', 'employment',\n",
              "       'installment_commitment', 'personal_status', 'other_parties',\n",
              "       'residence_since', 'property_magnitude', 'age', 'other_payment_plans',\n",
              "       'housing', 'existing_credits', 'job', 'num_dependents', 'own_telephone',\n",
              "       'foreign_worker', 'class'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bias Alert: Protected and Privileged Groups...\n",
        "\n",
        "Loan approval is a high risk application. We need to go very carefully as a result. We need to identify any protected characteristics (ie it would be illegal to differentiate based on these) present in the dataset. \n",
        "Our use-case is a loan approval use case, ethnicity and gender are protected charcteristics where biases would be illegal. We clearly need to remove these features.\n",
        "It is also possible to identify priviledged groups and remove biases based on these characterictics. \n"
      ],
      "metadata": {
        "id": "qV1jmvnXDdEl"
      },
      "id": "qV1jmvnXDdEl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Protected data items - 'personal_status' has a \"sex\" classifier - REMOVE!\n",
        "df = df_raw.drop(columns=['personal_status'])\n",
        "\n",
        "# Priviledged dataitems - remove all ages < 25\n",
        "df['age'] = df_raw['age'][lambda x: x >= 25]\n"
      ],
      "metadata": {
        "id": "0n2YS32BDd6N"
      },
      "id": "0n2YS32BDd6N",
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for biases on protected characteristics...\n",
        "df_female = df_raw[(df_raw['personal_status'].str.contains('female') != 0)]\n",
        "df_male = df_raw[(df_raw['personal_status'].str.contains('female') == 0)]\n",
        "\n",
        "# % of females \n",
        "female_good_credits = df_female[(df_female['class'] == 'good')].shape[0]\n",
        "female_good_credits_pct = female_good_credits / df_female.shape[0]\n",
        "#\n",
        "male_good_credits = df_male[(df_male['class'] == 'good')].shape[0] \n",
        "male_good_credits_pct = male_good_credits / df_male.shape[0]\n",
        "\n",
        "#  Difference in good credits for females and males...\n",
        "print('Female good credits: ' + str(format(round(female_good_credits_pct*100, 2))) + '%')\n",
        "print('Male good credits: ' + str(format(round(male_good_credits_pct*100, 2))) + '%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQaeBvyxMddk",
        "outputId": "b92fffde-9af6-40fb-943c-bc558e6f72df"
      },
      "id": "NQaeBvyxMddk",
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Female good credits: 64.84%\n",
            "Male good credits: 72.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is clearly a bias in the dataset, but to expand upon this we can use a Chi2 test to examine whether the observed \"bad\" credits we see in female rows, is statistically significant to the expected difference we see in male rows.\n",
        "\n",
        "It is possible that this bias is proxied by another data item in the dataset. **We will come back to this issue later**."
      ],
      "metadata": {
        "id": "cB4L-E2-Te1u"
      },
      "id": "cB4L-E2-Te1u"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Examine gender bias ... Generate  confusion matrix that to examine this bias...\n",
        "df_raw['gender'] = np.where(df_raw['personal_status'].str.contains('female') == 0, 'female', 'male')\n",
        "conf_matrix = pd.crosstab(df_raw['gender'], df_raw['class']) \n",
        "\n",
        "print('Male/Female Good/Bad Credits Confusion Matrix \\n' + str(conf_matrix))\n",
        "\n",
        "# Chi-square test of independence. \n",
        "c, p, dof, expected = chi2_contingency(conf_matrix) \n",
        "# Print the p-value\n",
        "print('\\n Chi-square test of independence')\n",
        "print('p-val of Chi2 test: ' + str(round(p, 4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPksuW6KKdhV",
        "outputId": "2cf7f86b-cb23-4bbe-d958-1387974fc2d8"
      },
      "id": "nPksuW6KKdhV",
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Male/Female Good/Bad Credits Confusion Matrix \n",
            "class   bad  good\n",
            "gender           \n",
            "female  191   499\n",
            "male    109   201\n",
            "\n",
            " Chi-square test of independence\n",
            "p-val of Chi2 test: 0.0207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we now have identified 2 classes, male and female, with males priviledged, which means we need to be fair between these two groups "
      ],
      "metadata": {
        "id": "fnCjWx6eqGHE"
      },
      "id": "fnCjWx6eqGHE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check for proxies of our protected characteristics\n",
        " \n",
        "We noted that there is a bias in our dataset between the protected characterisctis of gender, with male and female credits being different, and we removed this protected characteristic from the dataset to avoid this illegal bias in loan approval outcomes.\n",
        "\n",
        "Protected characteristics can be picked up (proxied) in other dataitems in more subtle ways though. For instance given that single parent households tend to disproportionately be led by a female adult, this may make 'num_dependents' a proxy for gender.\n",
        "\n",
        "It is important that we control for any possible protected biases, and one way of achieving this is to retrain our model using a mitigator, which trains by constraining the model weights to produce a balanced outcome between protected classes; male and female credits in this case.\n",
        "\n",
        "To ascertain proxies we can check the correlation of our protected feature with ther features in the dataset.\n",
        "\n",
        "Our protected features is categorical, so compare other categorical features with our protected feature we can use a Chi2 test. \n",
        "To compare  "
      ],
      "metadata": {
        "id": "tVRDgt03tG5c"
      },
      "id": "tVRDgt03tG5c"
    },
    {
      "cell_type": "code",
      "source": [
        "# MADHU: FAIRNESS FUNCTION HERE!!"
      ],
      "metadata": {
        "id": "eButjMDcPwHz"
      },
      "id": "eButjMDcPwHz",
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bdd49632",
      "metadata": {
        "id": "bdd49632"
      },
      "source": [
        "## Stage2b. Data Wrangling and Preprocessing\n",
        "\n",
        "Data Wrangling: As we have shown in previous chapters we need to convert categorical data into one-hot-encodings, clean characters from numeric data columns, carry out type conversions into numeric datatypes... The following cell shows the appropriate data wrangling to get our data into a good shape. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "id": "c62115a2",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "c62115a2",
        "outputId": "15b4fbb9-9f3d-4a91-a0b8-d121f0d16b2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     duration  credit_amount  installment_commitment  residence_since  age  \\\n",
              "0           6           1169                       4                4   67   \n",
              "1          48           5951                       2                2    0   \n",
              "2          12           2096                       2                3   49   \n",
              "3          42           7882                       2                4   45   \n",
              "4          24           4870                       3                4   53   \n",
              "..        ...            ...                     ...              ...  ...   \n",
              "995        12           1736                       3                4   31   \n",
              "996        30           3857                       4                4   40   \n",
              "997        12            804                       4                4   38   \n",
              "998        45           1845                       4                4    0   \n",
              "999        45           4576                       3                4   27   \n",
              "\n",
              "     existing_credits  num_dependents  foreign_worker  class  \\\n",
              "0                   2               1               1      0   \n",
              "1                   1               1               1      1   \n",
              "2                   1               2               1      0   \n",
              "3                   1               2               1      0   \n",
              "4                   2               2               1      1   \n",
              "..                ...             ...             ...    ...   \n",
              "995                 1               1               1      0   \n",
              "996                 1               1               1      0   \n",
              "997                 1               1               1      0   \n",
              "998                 1               1               1      1   \n",
              "999                 1               1               1      0   \n",
              "\n",
              "     checking_status_0<=X<200  ...  other_payment_plans_stores  \\\n",
              "0                           0  ...                           0   \n",
              "1                           1  ...                           0   \n",
              "2                           0  ...                           0   \n",
              "3                           0  ...                           0   \n",
              "4                           0  ...                           0   \n",
              "..                        ...  ...                         ...   \n",
              "995                         0  ...                           0   \n",
              "996                         0  ...                           0   \n",
              "997                         0  ...                           0   \n",
              "998                         0  ...                           0   \n",
              "999                         1  ...                           0   \n",
              "\n",
              "     housing_for free  housing_own  housing_rent  \\\n",
              "0                   0            1             0   \n",
              "1                   0            1             0   \n",
              "2                   0            1             0   \n",
              "3                   1            0             0   \n",
              "4                   1            0             0   \n",
              "..                ...          ...           ...   \n",
              "995                 0            1             0   \n",
              "996                 0            1             0   \n",
              "997                 0            1             0   \n",
              "998                 1            0             0   \n",
              "999                 0            1             0   \n",
              "\n",
              "     job_high qualif/self emp/mgmt  job_skilled  job_unemp/unskilled non res  \\\n",
              "0                                0            1                            0   \n",
              "1                                0            1                            0   \n",
              "2                                0            0                            0   \n",
              "3                                0            1                            0   \n",
              "4                                0            1                            0   \n",
              "..                             ...          ...                          ...   \n",
              "995                              0            0                            0   \n",
              "996                              1            0                            0   \n",
              "997                              0            1                            0   \n",
              "998                              0            1                            0   \n",
              "999                              0            1                            0   \n",
              "\n",
              "     job_unskilled resident  own_telephone_none  own_telephone_yes  \n",
              "0                         0                   0                  1  \n",
              "1                         0                   1                  0  \n",
              "2                         1                   1                  0  \n",
              "3                         0                   1                  0  \n",
              "4                         0                   1                  0  \n",
              "..                      ...                 ...                ...  \n",
              "995                       1                   1                  0  \n",
              "996                       0                   0                  1  \n",
              "997                       0                   1                  0  \n",
              "998                       0                   0                  1  \n",
              "999                       0                   1                  0  \n",
              "\n",
              "[1000 rows x 57 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0408a020-ab78-4fd4-81e6-d94a866a726c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>credit_amount</th>\n",
              "      <th>installment_commitment</th>\n",
              "      <th>residence_since</th>\n",
              "      <th>age</th>\n",
              "      <th>existing_credits</th>\n",
              "      <th>num_dependents</th>\n",
              "      <th>foreign_worker</th>\n",
              "      <th>class</th>\n",
              "      <th>checking_status_0&lt;=X&lt;200</th>\n",
              "      <th>...</th>\n",
              "      <th>other_payment_plans_stores</th>\n",
              "      <th>housing_for free</th>\n",
              "      <th>housing_own</th>\n",
              "      <th>housing_rent</th>\n",
              "      <th>job_high qualif/self emp/mgmt</th>\n",
              "      <th>job_skilled</th>\n",
              "      <th>job_unemp/unskilled non res</th>\n",
              "      <th>job_unskilled resident</th>\n",
              "      <th>own_telephone_none</th>\n",
              "      <th>own_telephone_yes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>1169</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>67</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>48</td>\n",
              "      <td>5951</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>2096</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42</td>\n",
              "      <td>7882</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24</td>\n",
              "      <td>4870</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>12</td>\n",
              "      <td>1736</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>30</td>\n",
              "      <td>3857</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>12</td>\n",
              "      <td>804</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>45</td>\n",
              "      <td>1845</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>45</td>\n",
              "      <td>4576</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 57 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0408a020-ab78-4fd4-81e6-d94a866a726c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0408a020-ab78-4fd4-81e6-d94a866a726c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0408a020-ab78-4fd4-81e6-d94a866a726c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ],
      "source": [
        "# list categorical columns...\n",
        "cat_cols =['checking_status', 'purpose', 'credit_history', 'savings_status', 'employment', 'other_parties', 'property_magnitude', 'other_payment_plans','housing','job','own_telephone']\n",
        "\n",
        "# One hot encoding of catagorical variables...\n",
        "df[cat_cols].astype(\"category\")\n",
        "one_hot_encodings = pd.get_dummies(df[cat_cols])\n",
        "\n",
        "#Combine dfs\n",
        "df = pd.concat([df, one_hot_encodings], axis=1)\n",
        "\n",
        "#remove categorical columns...\n",
        "df = df.drop(columns=cat_cols)\n",
        "\n",
        "#Data wrangling..... get types and bad values sorted out\n",
        "\n",
        "# Remove characters in numeric columns (note that the data type was set, by the open_csv function to object, \n",
        "# so we first convert to string before running the replace function (which can only be fun on str types)\n",
        "df['foreign_worker'] = df['foreign_worker'].str.replace('yes', '1')\n",
        "df['foreign_worker'] = df['foreign_worker'].str.replace('no', '0')\n",
        "df['class'] = df['class'].str.replace('good', '0')\n",
        "df['class'] = df['class'].str.replace('bad', '1')\n",
        "\n",
        "# Convert to numerics so we can use in ML... we force type conversions, then print our resulting df.\n",
        "df['foreign_worker'] = pd.to_numeric(df['foreign_worker'], errors ='coerce').fillna(0).astype('int')\n",
        "df['class'] = pd.to_numeric(df['class'], errors ='coerce').fillna(0).astype('int')\n",
        "df['duration'] = pd.to_numeric(df['duration'], errors ='coerce').fillna(0).astype('int')\n",
        "df['installment_commitment'] = pd.to_numeric(df['installment_commitment'], errors ='coerce').fillna(0).astype('int')\n",
        "df['residence_since'] = pd.to_numeric(df['residence_since'], errors ='coerce').fillna(0).astype('int')\n",
        "df['age'] = pd.to_numeric(df['age'], errors ='coerce').fillna(0).astype('int')\n",
        "df['num_dependents'] = pd.to_numeric(df['num_dependents'], errors ='coerce').fillna(0).astype('int')\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jCs33N2k8iVd"
      },
      "id": "jCs33N2k8iVd"
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "wljCC_8z_jL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c0e2e3-4e94-4945-a2c8-161f69883241"
      },
      "id": "wljCC_8z_jL2",
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['duration', 'credit_amount', 'installment_commitment',\n",
              "       'residence_since', 'age', 'existing_credits', 'num_dependents',\n",
              "       'foreign_worker', 'class', 'checking_status_0<=X<200',\n",
              "       'checking_status_<0', 'checking_status_>=200',\n",
              "       'checking_status_no checking', 'purpose_business',\n",
              "       'purpose_domestic appliance', 'purpose_education',\n",
              "       'purpose_furniture/equipment', 'purpose_new car', 'purpose_other',\n",
              "       'purpose_radio/tv', 'purpose_repairs', 'purpose_retraining',\n",
              "       'purpose_used car', 'credit_history_all paid',\n",
              "       'credit_history_critical/other existing credit',\n",
              "       'credit_history_delayed previously', 'credit_history_existing paid',\n",
              "       'credit_history_no credits/all paid', 'savings_status_100<=X<500',\n",
              "       'savings_status_500<=X<1000', 'savings_status_<100',\n",
              "       'savings_status_>=1000', 'savings_status_no known savings',\n",
              "       'employment_1<=X<4', 'employment_4<=X<7', 'employment_<1',\n",
              "       'employment_>=7', 'employment_unemployed', 'other_parties_co applicant',\n",
              "       'other_parties_guarantor', 'other_parties_none',\n",
              "       'property_magnitude_car', 'property_magnitude_life insurance',\n",
              "       'property_magnitude_no known property',\n",
              "       'property_magnitude_real estate', 'other_payment_plans_bank',\n",
              "       'other_payment_plans_none', 'other_payment_plans_stores',\n",
              "       'housing_for free', 'housing_own', 'housing_rent',\n",
              "       'job_high qualif/self emp/mgmt', 'job_skilled',\n",
              "       'job_unemp/unskilled non res', 'job_unskilled resident',\n",
              "       'own_telephone_none', 'own_telephone_yes'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage2c. Exploratory Data Analysis\n",
        "\n",
        "Once we have loaded the data and have it in a useable form, we now need to examine it, to build an intuition for the distributions, accuracy, missing values, imbalances and so on.\n",
        "\n"
      ],
      "metadata": {
        "id": "BrwhvERWDe_N"
      },
      "id": "BrwhvERWDe_N"
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_between_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKIZQIrJRJOF",
        "outputId": "36c9b184-1441-4e54-ede1-fd7ce41161a7"
      },
      "id": "OKIZQIrJRJOF",
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.correlation_between_features(X: pandas.core.frame.DataFrame)>"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd02130",
      "metadata": {
        "id": "1fd02130"
      },
      "source": [
        "One major bias, particularly when we are fitting a model to a relatively low probability event, such as a default, are imbalances in the data. By definition our target occurs less than half the time.\n",
        "\n",
        "#### Bias Alert: Imbalanced dataset\n",
        "\n",
        "Imbalances in datasets for classification problems are a big issue. We generally need to balance the dataset to contain an equal proportion of the different classes before training (and testing). For the credit use-case, we have two classes {1,0}, meaning that ideally 50% of our samples should be class=1; and 50% class=0. If this is not the case and we have an imbalance (we do), we can balance the data by up-sampling the minority class, or down-sampling the majority class.\n",
        "Let us first examine the dataset to determine whether it is in balance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "id": "9d772e1c",
      "metadata": {
        "id": "9d772e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "7733f575-3cde-46ac-c326-009dc5e45ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imbalanced dataset....\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-193-bf6f8674e07f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#run func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimbalanced_y_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-173-2d98a23163db>\u001b[0m in \u001b[0;36mimbalanced_y_check\u001b[0;34m(y, class_col)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Imbalanced dataset....'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pie'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Type\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mautopct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%1.1f%%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'class'"
          ]
        }
      ],
      "source": [
        "#run func\n",
        "imbalanced_y_check(df['class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0459bd5",
      "metadata": {
        "id": "e0459bd5"
      },
      "source": [
        "The dataset is not balanced. 70% of samples are class=0; only 30% are class=1. We should bring this into balance before we train our model, or risk introducing dangerous biases into our forecasts."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage3: Model Design \n",
        "\n",
        "Now we need to take what we have learned about the data, and find an appropriate model to achieve our KPIs."
      ],
      "metadata": {
        "id": "i9Vx9c_4sStE"
      },
      "id": "i9Vx9c_4sStE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 3a: Test the performance of different up, and down sampling approaches"
      ],
      "metadata": {
        "id": "c0ubSKhBH-pg"
      },
      "id": "c0ubSKhBH-pg"
    },
    {
      "cell_type": "markdown",
      "id": "e286323c",
      "metadata": {
        "id": "e286323c"
      },
      "source": [
        "Before we start up or down-sampling to correct the imbalance, we first we need to create our testing and training datasets. We can then balance the training set. This is to keep the training-set in-sample and the testing-set strictly out-of-sample. There is a risk of data leakage in this process we must consider and contriol for.\n",
        "\n",
        "#### Bias Alert: Data Leakage \n",
        "\n",
        "Separate training and testing datasets BEFORE balancing the dataset is crucial to avoid adta leakage. This is crucial as our learner must not see any of the test samples until we actually test it for performance. If we fail to separate testing and training data before up samplng, we can suffer data-snooping biases (also called data-leakage), which would invalidate our model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "eTjDqiD5f_j1"
      },
      "id": "eTjDqiD5f_j1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9517124",
      "metadata": {
        "id": "f9517124"
      },
      "outputs": [],
      "source": [
        "# Define X and y variables\n",
        "cols  = list(df.columns)\n",
        "cols.remove('class')\n",
        "\n",
        "# Contains only numerics\n",
        "X = df[cols]\n",
        "y = df['class']\n",
        "\n",
        "#Test and train set    \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3f79ba0",
      "metadata": {
        "id": "d3f79ba0"
      },
      "source": [
        "### Stage Test up and down sampling approaches using a Random Forest Classifier\n",
        "We will use the RandomForest classifier to help us test different up and down sampling approaches to deal with the imbalanced dataset. We will be able to see the relative performance of each balancing approach on our problem.\n",
        "\n",
        "First let us run the classifier on the imbalanced data and examine the F1 score that results when we test the model. (It is a very poor result)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c02db5bb",
      "metadata": {
        "id": "c02db5bb"
      },
      "source": [
        "Get the sklearn packages we will need for our clasification problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af12e96b",
      "metadata": {
        "id": "af12e96b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# metrics...\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7484849",
      "metadata": {
        "id": "c7484849"
      },
      "source": [
        "We can wrap training for the Random Forest classifier, and the printing of performance metrics in a function, as we will be running this more than once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c09b7144",
      "metadata": {
        "id": "c09b7144"
      },
      "outputs": [],
      "source": [
        "# Declare a function to wrap training of a classifier and printing of performance data\n",
        "def run_rf_classification_models(X_train: pd.DataFrame, \n",
        "                              X_test: pd.DataFrame, \n",
        "                              y_train: pd.DataFrame, \n",
        "                              y_test: pd.DataFrame) -> object:\n",
        "    \n",
        "    '''\n",
        "    Args:\n",
        "      X_train: DataFrame with training data for classifier, columns are features, rows are instances\n",
        "      X_test: Test data matching above shape\n",
        "      y_train: training data target variable {1,0}, instances are rows.\n",
        "      y_test: test data target variable {1,0}, instances are rows.\n",
        "       \n",
        "    Returns:\n",
        "       rf: sklearn model object\n",
        "       \n",
        "    Author:\n",
        "       Dan Philps\n",
        "    '''\n",
        "\n",
        "    #sanity\n",
        "    if X_train.shape[0] != y_train.shape[0]:\n",
        "      raise TypeError('Bad parameter: X_train.shape[0] != y_train.shape[0]')\n",
        "    if X_test.shape[0] != y_test.shape[0]:\n",
        "      raise TypeError('Bad parameter: X_train.shape[0] != y_train.shape[0]')\n",
        "    if (X_train.dtypes != X_test.dtypes).sum() != 0:\n",
        "      raise TypeError('Bad parameter: X_train.dtype != X_test.dtype')\n",
        "    if (y_train.dtypes != y_test.dtypes):\n",
        "      raise TypeError('Bad parameter: y_train.dtype != y_test.dtype')\n",
        "\n",
        "    # Scale and transform the data for training\n",
        "    sclr = StandardScaler()\n",
        "    sclr.fit(X_train) # scale to 0 mean and std dev 1 on training data\n",
        "\n",
        "    X_train = sclr.fit_transform(X_train) # scale both sets:\n",
        "    X_test = sclr.fit_transform(X_test)\n",
        "\n",
        "    # classifier train\n",
        "    rf = RandomForestClassifier(max_depth=5,random_state=0)\n",
        "    rf.fit(X_train,y_train)\n",
        "    y_train_hat =rf.predict(X_train)\n",
        "    y_test_hat = rf.predict(X_test)\n",
        "\n",
        "    # Print score\n",
        "    print(type(rf))        \n",
        "    print(f\"Accuracy train: {rf.score(X_train,y_train):.4f}, test: \",\n",
        "      f\"{rf.score(X_test,y_test):.4f}\")\n",
        "    print(f\"Precision train: {precision_score(y_train, y_train_hat, average=None)[0]:.4f}, test: \",\n",
        "      f\"{precision_score(y_test,y_test_hat, average=None)[0]:.4f}\")\n",
        "    print(f\"Recall train: {recall_score(y_train, y_train_hat, average=None)[0]:.4f}, test: \",\n",
        "      f\"{recall_score(y_test,y_test_hat, average=None)[0]:.4f}\")\n",
        "    print(f\"F1 train: {f1_score(y_train, y_train_hat, average=None)[1]:.4f}, test: \",\n",
        "      f\"{f1_score(y_test,y_test_hat, average=None)[1]:.4f}\")\n",
        "    \n",
        "    #Print confusion matrix...\n",
        "    cf_matrix = confusion_matrix(y_test, y_test_hat, labels=[0, 1]) \n",
        "    cf_matrix_norm = cf_matrix.astype('float') # / cf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    ax = sns.heatmap(cf_matrix_norm, annot=True, cmap='Blues', fmt='g')\n",
        "    ax.set_title('Confusion Matrix\\n\\n');\n",
        "    ax.set_xlabel('\\nPredicted Values')\n",
        "    ax.set_ylabel('Actual Values ');\n",
        "    plt.show()\n",
        "\n",
        "    #sanity\n",
        "    if rf is None:\n",
        "      raise TypeError('Bad return: rf is None')\n",
        "\n",
        "    return rf\n",
        "\n",
        "#run our classifier function\n",
        "mod = run_rf_classification_models(X_train, X_test, y_train, y_test)\n",
        "\n",
        "print('Now use sklearns predict_proba to generate probability values for each prediction')\n",
        "mod.predict_proba(X_test)[0:9,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8166fcf",
      "metadata": {
        "id": "f8166fcf"
      },
      "source": [
        "### 3a i) Upsampling using resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4ef22f5",
      "metadata": {
        "id": "a4ef22f5"
      },
      "source": [
        "First we test up-sampling using sklearn's resample, and examine how well it does using the RandomForest classifier. Resampling can up-sample by simply randomly selecting and copying existing observations of the minority class. We can balance the classes in this way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "739e0d7d",
      "metadata": {
        "id": "739e0d7d"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "#Split first to avoid data-snooping\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
        "\n",
        "# Create up-sampled data set for minority class\n",
        "# note that n_samples= the number of samples the imbalance represents.\n",
        "X_upsampled, y_upsampled = resample(X_train[y_train == 1],\n",
        "                                        y_train[y_train == 1],\n",
        "                                        replace=True,\n",
        "                                        n_samples=(X_train[y_train == 0].shape[0]-X_train[y_train == 1].shape[0]),\n",
        "                                        random_state=None)\n",
        "\n",
        "#Combine train with upsampled\n",
        "X_upsampled = X_train.append(X_upsampled)\n",
        "y_upsampled = y_train.append(y_upsampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc1e5726",
      "metadata": {
        "id": "fc1e5726"
      },
      "source": [
        "Let us check everything is in balance now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8cb61d7",
      "metadata": {
        "id": "b8cb61d7"
      },
      "outputs": [],
      "source": [
        "#get pie of the current imbalance\n",
        "#temp = pd.concat([X_upsampled, y_upsampled], axis=1)\n",
        "#imbalanced_y_check(temp['class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53dea8e1",
      "metadata": {
        "id": "53dea8e1"
      },
      "source": [
        "Dataset is perfectly in balance..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5caebbb0",
      "metadata": {
        "id": "5caebbb0"
      },
      "outputs": [],
      "source": [
        "#Run our function....\n",
        "model = run_rf_classification_models(X_upsampled, X_test, y_upsampled, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b4afcc1",
      "metadata": {
        "id": "1b4afcc1"
      },
      "source": [
        "The F1 score on the test data has increased markedly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e98b384e",
      "metadata": {
        "id": "e98b384e"
      },
      "source": [
        "### 3a ii) Up-sampling using a synthetic over sampling approach called SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "235505f6",
      "metadata": {
        "id": "235505f6"
      },
      "source": [
        "Add the libraries we will need... and generate the synthetic data to balance our classes using SMOTE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dadf40d",
      "metadata": {
        "id": "5dadf40d"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "#How many samples do we need to balance?\n",
        "idx = np.random.choice(X_train.shape[0], size=X_train[y_train == 0].shape[0]-X_train[y_train == 1].shape[0], replace=False)\n",
        "\n",
        "# Generate SMOTE samples and use this to train\n",
        "upsampler_smote = SMOTE()\n",
        "X_upsampled_smote, y_upsampled_smote = upsampler_smote.fit_resample(X=X_train, y=y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a14d264",
      "metadata": {
        "id": "4a14d264"
      },
      "source": [
        "Before we use the up-sampled dataset to train our classifier, let us first examine the distribution of the synthetic datapoints that SMOTE creates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff54d69",
      "metadata": {
        "id": "4ff54d69"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7)) \n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(X_train['credit_amount'],X_train['age'], X_train['duration'], marker=\"o\", s=10, c='blue', label='Real datapoints')\n",
        "ax.scatter(X_upsampled_smote['credit_amount'],X_upsampled_smote['age'], X_upsampled_smote['duration'], marker=\"+\", s=50, c='red', label='SMOTE datapoints')\n",
        "\n",
        "# set axes range\n",
        "plt.xlim(-500, 11000)\n",
        "plt.ylim(0, 60)\n",
        "\n",
        "ax.set_xlabel('credit_amount')\n",
        "ax.set_ylabel('age')\n",
        "ax.set_zlabel('duration')\n",
        "\n",
        "plt.title('How SMOTE Samples are Distributed vs Real Data Points')\n",
        "plt.legend(loc=1,framealpha=1, fontsize=8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a875103b",
      "metadata": {
        "id": "a875103b"
      },
      "source": [
        "The synthetic datapoints look realistic at a glance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c031433f",
      "metadata": {
        "id": "c031433f"
      },
      "outputs": [],
      "source": [
        "#get pie of the current imbalance\n",
        "temp = pd.concat([X_upsampled_smote, y_upsampled_smote], axis=1)\n",
        "imbalanced_y_check(temp['class'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(temp['class'])"
      ],
      "metadata": {
        "id": "V2Ky9T8DXIAj"
      },
      "id": "V2Ky9T8DXIAj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "89d8fdc6",
      "metadata": {
        "id": "89d8fdc6"
      },
      "source": [
        "After up-sampling using SMOTE's synthetic data, the dataset is perfectly in balance... however we have a Fairness problem. Can you spot what it is?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "750a6983",
      "metadata": {
        "id": "750a6983"
      },
      "outputs": [],
      "source": [
        "#Run our function....\n",
        "model = run_rf_classification_models(X_upsampled_smote, X_test, y_upsampled_smote, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30066c4b",
      "metadata": {
        "id": "30066c4b"
      },
      "source": [
        "#### Synthetic Data's Fairness Banana Skin - Data Leakage of protected characteristics\n",
        "\n",
        "Remember that we had two distinct groups we need to ensure fairness for, females and males? We just: generated synthetic data based off the entire dataset though - using both male and female samples - and in doing so we polluted the new rows we created with both male and female data characteristics. \n",
        "We therefore need to generate synthetic data for male rows and THEN generate synthetic data for female rows avoiding data leakage of protected characteristics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Generate SMOTE samples and use this to train... do for each protected class \n",
        "# separately and THEN combine.\n",
        "upsampler_smote = SMOTE()\n",
        "\n",
        "\n",
        "#** First add back the gender column as after the train_test_split we want to know which row is which.\n",
        "#\n",
        "X_gender = pd.concat([X, df_raw['gender']], axis=1)\n",
        "y_gender = pd.concat([y, df_raw['gender']], axis=1)\n",
        "\n",
        "#Split first to avoid data-snooping\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_gender, y_gender, test_size=0.3, random_state=None)\n",
        "\n",
        "# GENDER: FIRST we generate synthetic samples for females, where all these synthetic \n",
        "# samples will be exclusively generated from female instances.\n",
        "X_train_female = X_train[(X_train['gender'] == 'female')]\n",
        "X_train_female = X_train_female.drop('gender', axis=1)\n",
        "#\n",
        "y_train_female = y_train[(y_train['gender'] == 'female')]\n",
        "y_train_female = y_train_female.drop('gender', axis=1)\n",
        "\n",
        "X_upsampled_smote_female, y_upsampled_smote_female = upsampler_smote.fit_resample(X=X_train_female, y=y_train_female)\n",
        "\n",
        "#GENDER: SECOND males...where all these synethic \n",
        "# samples will be exclusively generaqted from male instances.\n",
        "X_train_male = X_train[(X_train['gender'] == 'male')]\n",
        "X_train_male = X_train_male.drop('gender', axis=1)\n",
        "#\n",
        "y_train_male = y_train[(y_train['gender'] == 'male')]\n",
        "y_train_male = y_train_male.drop('gender', axis=1)\n",
        "\n",
        "X_upsampled_smote_male, y_upsampled_smote_male = upsampler_smote.fit_resample(X=X_train_male, y=y_train_male)\n",
        "\n",
        "#** Now remove the gender column...\n",
        "X_train = X_train.drop('gender', axis=1)\n",
        "y_train = y_train.drop('gender', axis=1)\n",
        "\n",
        "# Combine male and female training data, including the SMOTE data\n",
        "X_upsampled_smote = pd.concat([X_upsampled_smote_female, X_upsampled_smote_male], axis=0) # Note the axis is set to zero, so we are adding rows the bottom of the df\n",
        "y_upsampled_smote = pd.concat([y_upsampled_smote_female, y_upsampled_smote_male], axis=0)"
      ],
      "metadata": {
        "id": "TTAnktvT73Ql"
      },
      "id": "TTAnktvT73Ql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine the male and female upsampled data..."
      ],
      "metadata": {
        "id": "ByQ3478C7nXx"
      },
      "id": "ByQ3478C7nXx"
    },
    {
      "cell_type": "code",
      "source": [
        "X_upsampled_smote = X_upsampled_smote_female + X_upsampled_smote_male\n",
        "y_upsampled_smote = y_upsampled_smote_female + y_upsampled_smote_male"
      ],
      "metadata": {
        "id": "vNHwc1QP7wCb"
      },
      "id": "vNHwc1QP7wCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get pie of the current imbalance\n",
        "temp = pd.concat([X_upsampled_smote, y_upsampled_smote], axis=1)\n",
        "imbalanced_y_check(pd.DataFrame(temp['class']))"
      ],
      "metadata": {
        "id": "GuXSRiAd7n95"
      },
      "id": "GuXSRiAd7n95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(temp['class'])"
      ],
      "metadata": {
        "id": "W2R8SeLHX-ZV"
      },
      "id": "W2R8SeLHX-ZV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3a iii) Down-sampling - Removing rows to balance the classes"
      ],
      "metadata": {
        "id": "kHUCz-3ksp6r"
      },
      "id": "kHUCz-3ksp6r"
    },
    {
      "cell_type": "markdown",
      "id": "0a94d7e4",
      "metadata": {
        "id": "0a94d7e4"
      },
      "source": [
        "Now we can test down-sampling, which is simply removing samples from the majority class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dbc9854",
      "metadata": {
        "id": "8dbc9854"
      },
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler \n",
        "\n",
        "#Split first to avoid data-snooping\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
        "\n",
        "# Randomly downsample rows in the majority class\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_downsampled, y_downsampled = rus.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dk5pDaJUsO1k"
      },
      "id": "dk5pDaJUsO1k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0beacf7",
      "metadata": {
        "id": "c0beacf7"
      },
      "outputs": [],
      "source": [
        "#get pie of the current imbalance\n",
        "temp = pd.concat([X_downsampled, y_downsampled], axis=1)\n",
        "imbalanced_y_check(temp['class'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp['class']"
      ],
      "metadata": {
        "id": "nsVXCSA3Yf2D"
      },
      "id": "nsVXCSA3Yf2D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6de0276",
      "metadata": {
        "id": "d6de0276"
      },
      "outputs": [],
      "source": [
        "#Run our function....\n",
        "model = run_rf_classification_models(X_downsampled, X_test, y_downsampled, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e6ea491",
      "metadata": {
        "id": "5e6ea491"
      },
      "source": [
        "As we can see, all up and down sampling approaches have outperformed the F1 Score on the imbalanced data. The most impressive performance in this case is from up-sampling using SMOTE synthetic data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66bfe3c2",
      "metadata": {
        "id": "66bfe3c2"
      },
      "source": [
        "## Stage 4b: Model Selection\n",
        "\n",
        "\n",
        "Next we need to select the type of classifier we are going to use to model our problem and separate good from bad credits. We could just make a heuristic selection, but we are likely to be biased in making this selection. \n",
        "\n",
        "#### Bias Alert: Availability Heuristic\n",
        "For instance,  is our quant team overpopulated by linear regression experts? Do we have a greater level of familiarity for Random Forest classifiers?\n",
        "\n",
        "One way of dealing with this is to use of develop a model selection approach. We can test the problem, using many different classsifiers and assess the performance of exach based on our KPIs. However, we must be careful as even this more exhaustive approach is also open to biases. For instance if we are selection models based on p-values, we would suffer from multiplicity bias where testing multiple approaches would exaggerate the significance of a success. \n",
        "\n",
        "#### Bias Alert: Multiplicity bias\n",
        "\n",
        "To control for this we would use a measure such as F1-score, and we would test this on test samples not used for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test and train set    \n",
        "X_train_cv, X_cv, y_train_cv, y_cv = train_test_split(X_train, y_train, test_size=0.3, random_state=None)"
      ],
      "metadata": {
        "id": "tUbobdmKR9Or"
      },
      "id": "tUbobdmKR9Or",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sklean provides many different classifiers and we will be testing a wide range to determine their accuracy on our problem."
      ],
      "metadata": {
        "id": "5SUdXN-jhrza"
      },
      "id": "5SUdXN-jhrza"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Classsifiers from sklearn\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier \n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Performance metrics...\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score"
      ],
      "metadata": {
        "id": "G3Dro5K_QkgZ"
      },
      "id": "G3Dro5K_QkgZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address the availability heuristic we will now build a function that tests a number of different classifiers on our problem, and the one with the best accuracy on the cross-validation data, we will select as the \"best\"."
      ],
      "metadata": {
        "id": "JElvKh2qh75k"
      },
      "id": "JElvKh2qh75k"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Func to wrap up running these selected classification learners...\n",
        "# NOTE: to test the performance of the learners out-of-sample, we should use a cross-validation dataset\n",
        "# this is a hold back dataset and we will use our testing data to do this, in this case. \n",
        "def auto_classifier_selection(X_train: pd.DataFrame, \n",
        "                            X_cross_validation: pd.DataFrame, \n",
        "                            y_train: pd.DataFrame, \n",
        "                            y_cross_validation: pd.DataFrame) -> (object, list, list, list):\n",
        "    \n",
        "    '''\n",
        "    Args:\n",
        "      X_train: DataFrame with training data for classifier, columns are features, rows are instances\n",
        "      X_cross_validation: cross validation data matching above shape, used in model selection\n",
        "      y_train: training data target variable {1,0}, instances are rows.\n",
        "      y_cross_validation: test data target variable {1,0}, instances are rows, used in model selection\n",
        "       \n",
        "    Returns:\n",
        "       max_mdl: sklearn model object performing \"best\"\n",
        "       all_mdls: list of sklearn classifier objects trained\n",
        "       all_mdls_desc: list of description of the above model objects (elements corresponding to the above also)\n",
        "       all_mdls_F1score: list of F2 scores of the above model objects (elements corresponding to the above also)\n",
        "       \n",
        "    Author:\n",
        "       Dan Philps\n",
        "    '''\n",
        "    #sanity\n",
        "    if X_train.shape[0] != y_train.shape[0]:\n",
        "      raise TypeError('Bad parameter: X_train.shape[0] != y_train.shape[0]')\n",
        "    if X_cross_validation.shape[0] != y_cross_validation.shape[0]:\n",
        "      raise TypeError('Bad parameter: X_train.shape[0] != y_train.shape[0]')\n",
        "    if (X_train.dtypes != X_cross_validation.dtypes).sum() != 0:\n",
        "      raise TypeError('Bad parameter: X_train.dtype != X_cross_validation.dtype')\n",
        "    if (y_train.dtypes != y_cross_validation.dtypes):\n",
        "      raise TypeError('Bad parameter: y_train.dtype != y_cross_validation.dtype')\n",
        "\n",
        "    #Balance training data....\n",
        "    # Generate SMOTE samples and use this to train\n",
        "    upsampler_smote = SMOTE()\n",
        "    X_upsampled_smote, y_upsampled_smote = upsampler_smote.fit_resample(X_train.values, y_train.values)\n",
        "\n",
        "    sclr = StandardScaler()\n",
        "    sclr.fit(X_train.values) # scale to 0 mean and std dev 1 on training data\n",
        "\n",
        "    X_train = sclr.fit_transform(X_upsampled_smote) # scale both sets:\n",
        "    X_cross_validation = sclr.fit_transform(X_cross_validation)\n",
        "    \n",
        "    # These are the classifiers we will select from...\n",
        "    dtc = DecisionTreeClassifier(max_depth=5) #If we allow endless depth we overfit\n",
        "    gnb = GaussianNB()\n",
        "    lr = LogisticRegression(max_iter=2000,random_state=0)\n",
        "    mlp = MLPClassifier(max_iter=2000,random_state=1, early_stopping=True) # MLP will tend to overfit unless we stop early   \n",
        "    rf = RandomForestClassifier(max_depth=3,random_state=0) # << artibitrary parameters, consider hyper parameter tuning.\n",
        "    lda = LinearDiscriminantAnalysis()\n",
        "    qda = QuadraticDiscriminantAnalysis()\n",
        "    ada = AdaBoostClassifier()\n",
        "    gbc = GradientBoostingClassifier()\n",
        "    knn = KNeighborsClassifier(n_neighbors=3) # << artibitrary parameters, consider hyper parameter tuning.\n",
        "    svc = SVC(kernel=\"rbf\", C=0.025, probability=True) # << artibitrary parameters, consider hyper parameter tuning.\n",
        "    nsvc = NuSVC(probability=True)\n",
        "    \n",
        "    all_mdls = [dtc, gnb, lr, mlp, rf, lda, qda, ada, gbc, knn, svc, nsvc]\n",
        "    all_mdls_desc = ['dtc', 'gnb', 'lr', 'mlp', 'rf', 'lda', 'qda', 'ada', 'gbc', 'knn', 'svc', 'nsvc']\n",
        "    all_mdls_F1score = []\n",
        "    \n",
        "    # Loop through each classifer and record the \"best\"...\n",
        "    max_f1 = 0\n",
        "    for mdl in all_mdls:\n",
        "        #Fit model\n",
        "        mdl.fit(X_upsampled_smote,y_upsampled_smote)  \n",
        "        y_train_hat = mdl.predict(X_upsampled_smote)\n",
        "        y_cross_validation_hat = mdl.predict(X_cross_validation)       \n",
        "        mdl.predict_proba(X_cross_validation)\n",
        "\n",
        "        # Output model selection information....\n",
        "        print(mdl)\n",
        "        print(f\"F1 train: {f1_score(y_upsampled_smote, y_train_hat, average=None)[1]:.4f}, cross-validation: \",\n",
        "        f\"{f1_score(y_cross_validation,y_cross_validation_hat, average=None)[1]:.4f}\")\n",
        "  \n",
        "        # Selection based on cross-validation set, ie out of sample data not used in training\n",
        "        this_cv_f1 = f1_score(y_cross_validation,y_cross_validation_hat, average=None)[1]\n",
        "        if this_cv_f1 > max_f1:\n",
        "            max_f1 = this_cv_f1\n",
        "            max_mdl = mdl\n",
        "        \n",
        "        #Save the F1 score of this model...\n",
        "        all_mdls_F1score.append(this_cv_f1)\n",
        "\n",
        "    # The best....\n",
        "    #Fit...\n",
        "    max_mdl.fit(X_upsampled_smote,y_upsampled_smote)\n",
        "    y_train_hat = max_mdl.predict(X_upsampled_smote)\n",
        "    y_cross_validation_hat = max_mdl.predict(X_cross_validation)\n",
        "    \n",
        "    #    Print score\n",
        "    print('\\nWinner\\n', type(max_mdl))        \n",
        "    print(f\"Accuracy train: {max_mdl.score(X_train,y_upsampled_smote):.4f}, cross-validation: \",\n",
        "      f\"{max_mdl.score(X_cross_validation,y_cross_validation):.4f}\")\n",
        "    print(f\"Precision train: {precision_score(y_upsampled_smote, y_train_hat, average=None)[0]:.4f}, cross-validation: \",\n",
        "      f\"{precision_score(y_cross_validation,y_cross_validation_hat, average=None)[0]:.4f}\")\n",
        "    print(f\"Recall train: {recall_score(y_upsampled_smote, y_train_hat, average=None)[0]:.4f}, cross-validation: \",\n",
        "      f\"{recall_score(y_cross_validation,y_cross_validation_hat, average=None)[0]:.4f}\")\n",
        "    print(f\"F1 train: {f1_score(y_upsampled_smote, y_train_hat, average=None)[1]:.4f}, cross-validation: \",\n",
        "      f\"{f1_score(y_cross_validation,y_cross_validation_hat, average=None)[1]:.4f}\")\n",
        "        \n",
        "    #Print confusion matrix...\n",
        "    cf_matrix = confusion_matrix(y_cross_validation, y_cross_validation_hat, labels=[0, 1]) \n",
        "    cf_matrix_norm = cf_matrix.astype('float')\n",
        "\n",
        "    ax = sns.heatmap(cf_matrix_norm, annot=True, cmap='Blues', fmt='g')\n",
        "    ax.set_title('Confusion Matrix\\n\\n');\n",
        "    ax.set_xlabel('\\nPredicted Values')\n",
        "    ax.set_ylabel('Actual Values ');\n",
        "    plt.show()\n",
        "    \n",
        "    #sanity\n",
        "    if max_mdl is None:\n",
        "      raise TypeError('Bad return: max_mdl is None')\n",
        "\n",
        "    return max_mdl, all_mdls, all_mdls_desc, all_mdls_F1score\n",
        "\n",
        "# Run our function....autoselect the best classifier wrt F1\n",
        "max_mdl, all_models, all_models_desc, all_mdls_F1score = auto_classifier_selection(X_train_cv, X_cv, y_train_cv, y_cv)\n"
      ],
      "metadata": {
        "id": "cyji4ydlQTVZ"
      },
      "id": "cyji4ydlQTVZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output from our model selection function shows how each classifier performed on the training and cross-validation datasets. We can now test the \"winning classifier\" on our test data to ensure the cross-validation tests were effective in selecting a good performance out of sample."
      ],
      "metadata": {
        "id": "OHdKdhL6MF42"
      },
      "id": "OHdKdhL6MF42"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict\n",
        "y_test_hat = max_mdl.predict(X_test.values)\n",
        "\n",
        "# Analyst KPI...\n",
        "f1, prec, rec = kpi_review_analyst(mdl=max_mdl,X=X_test, y=y_test[:], y_hat=y_test_hat[:])\n",
        "print(f1, prec, rec)"
      ],
      "metadata": {
        "id": "BuJjXTsYMbBs"
      },
      "id": "BuJjXTsYMbBs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bias Alert: Inductive bias\n",
        "\n",
        "Even in the case where we select the most appropriate model, we may still be exposed to inductive biases. Each learner has a specific way that will approximate the function we are attempting to approximate, in this cadse the function of credit quality based on the characteristics of borrowers. Decision Trees, for instance, have inductive biases associated with greedy separation, whereas Random Forests (RF) mitigate this bias by using many randomized decision trees, but in turn introduce (lesser) inductive biases associated with the way an RF's underlying decision trees are contructed. One mitigant is to ensemble different learners, using a soft-max function or a voting approach. For voting based ensembling we would simply run a number of classifiers, take the majority answer: good/bad credit. (Note that we have included \n"
      ],
      "metadata": {
        "id": "RM6iBjWd_yEn"
      },
      "id": "RM6iBjWd_yEn"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run the challenger ensemble\n",
        "y_test_hat, ens_mdl = challenger_ensemble_run(all_mdls=all_models,\n",
        "                        all_mdls_desc=all_models_desc,\n",
        "                        all_mdls_F1score=all_mdls_F1score,\n",
        "                        X_train=X_train\n",
        "                        y_train=y_train\n",
        "                        X_test=X_test)\n",
        "\n",
        "# Analyst KPI...\n",
        "f1, prec, rec = kpi_review_analyst(mdl=ens_mdl,X=X_test, y=y_test[:], y_hat=y_test_hat[:])\n",
        "print(f1, prec, rec)"
      ],
      "metadata": {
        "id": "xjkMTtAW22C6"
      },
      "id": "xjkMTtAW22C6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DRAFT!!!! FAIRNESS FUNCTIONS Removing features correlated with protected characteristics"
      ],
      "metadata": {
        "id": "wfSaSlGfw66d"
      },
      "id": "wfSaSlGfw66d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us inspect the results between the \"fair\" model outcomes and the potentially biased...."
      ],
      "metadata": {
        "id": "cVV8YHYuwfRU"
      },
      "id": "cVV8YHYuwfRU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 4c: KPI Checks - Most importantly checks for.\n",
        "\n",
        "We noted at the outset that there was a bias between the protected characterisctis of gender, with male and female credits being different, and we removed this protected characteristic from the dataset to avoid this illegal bias in loan approval outcomes.\n",
        "\n",
        "Protected characteristics can be picked up (proxied) in other dataitems in more subtle ways though. For instance given that single parent households tend to disproportionately be led by a female adult, this may make 'num_dependents' a proxy for gender.\n",
        "\n",
        "It is important that we control for any possible protected biases, and one way of achieving this is to retrain our model using a mitigator, which trains by constraining the model weights to produce a balanced outcome between protected classes; male and female credits in this case."
      ],
      "metadata": {
        "id": "LvIWjJXQtAVd"
      },
      "id": "LvIWjJXQtAVd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
        "Madhu... Fairness applied to the chosen model....\n",
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ "
      ],
      "metadata": {
        "id": "SXogafeXHFQS"
      },
      "id": "SXogafeXHFQS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The balance almost equal, this indicates that our model is tending to show now bias based non gender, which indicates that removing the protected dataitem in thefirst place has had a balancing effect on the outcomes. \n",
        "\n",
        "However, we can still explicuitly remove correlated features, and constrain model training to remove bias."
      ],
      "metadata": {
        "id": "6efw9jQlqDse"
      },
      "id": "6efw9jQlqDse"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage4: Model Deployment \n",
        "\n",
        "Model deployment generally involves a change process, several levels of testing and sign off, asignment of responsibilities for the live operation of the process, models and data before deployiong the code to the cloud (or on native hardware). \n",
        "A key part of thius stage is communication of the KPIs to stakleholders to enable them to understand the way the models operate, the risks involved and to be accountable for deploying the models\n"
      ],
      "metadata": {
        "id": "0LFyHIAP59O7"
      },
      "id": "0LFyHIAP59O7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Communication: Stakeholder Oriented Explanations\n",
        "\n",
        "Communication of how the model has reached the outcomes it is has, is crutial to achieve fairness, transparency, accountability, and trust in the whole process. Each stakeholder in the process needs to see different elements. The Data Scientist and technical leadership need to review the nut and bolts of the model, reviewing residuals plots, parameter importance, interaction terms and many other metrics. The customer needs to see far less information, and mainly that associated with a refusal of credit. Compliance resources and regulators need to see something different again, such as fairness regarding protected characteristics, the accuracy and therefore capital risk represented by the models. \n",
        "In this section we look at stakeholder oriented explanations and we will be using standard charts of important analytics, such as residual plots, and SHAP.\n",
        "\n",
        "First let us get the packages we will need..."
      ],
      "metadata": {
        "id": "JZGI4nBO0HlI"
      },
      "id": "JZGI4nBO0HlI"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "4dWv02uvQWyd"
      },
      "id": "4dWv02uvQWyd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap"
      ],
      "metadata": {
        "id": "Nx16rPvNQdrZ"
      },
      "id": "Nx16rPvNQdrZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now instantiate the SHAP explainer object for our classifier and generate Shapley values for the test data... Note that as we will be using the test data, all the analysis is therefore based on the out of sample performance of our model."
      ],
      "metadata": {
        "id": "Ku_I-kcG8bpR"
      },
      "id": "Ku_I-kcG8bpR"
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(max_mdl).__name__)\n",
        "\n",
        "# Instantiate an explainer object for our chosen classifier...\n",
        "if type(max_mdl).__name__ == 'DecisionTreeClassifier':\n",
        "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
        "  shap_values = explainer(X_test.values)\n",
        "elif type(max_mdl).__name__ == 'GaussianNB':\n",
        "  explainer = shap.KernelExplainer(max_mdl.predict, X_test.values)\n",
        "  shap_values = explainer.shap_values(X_test.values)\n",
        "elif type(max_mdl).__name__ == 'LogisticRegression':\n",
        "  masker=shap.maskers.Impute(data=X_test) #,feature_names=X_test.columns)\n",
        "  explainer = shap.LinearExplainer(max_mdl, masker = masker)  \n",
        "  shap_values = explainer.shap_values(X_test.values)\n",
        "elif type(max_mdl).__name__ == 'MLPClassifier':\n",
        "  explainer = shap.KernelExplainer(max_mdl.predict, X_test.values)\n",
        "  shap_values = explainer.shap_values(X_test.values)\n",
        "elif type(max_mdl).__name__ == 'RandomForestClassifier':\n",
        "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
        "  shap_values = explainer(X_test.values)\n",
        "elif type(max_mdl).__name__ == 'LinearDiscriminantAnalysis':\n",
        "  masker = shap.maskers.Independent(data = X_test.values)\n",
        "  explainer = shap.LinearExplainer(max_mdl, masker = masker)\n",
        "  shap_values = explainer(X_test.values)\n",
        "elif type(max_mdl).__name__ == 'QuadraticDiscriminantAnalysis':\n",
        "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
        "  shap_values = explainer(X_test.values)\n",
        "elif type(max_mdl).__name__ == 'AdaBoostClassifier':\n",
        "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
        "  shap_values = explainer(X_test.values)\n",
        "elif type(max_mdl).__name__ == 'GradientBoostingClassifier':\n",
        "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
        "  shap_values = explainer(X_test.values)\n",
        "elif type(max_mdl).__name__ == 'KNeighborsClassifier':\n",
        "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
        "  shap_values = explainer(X_test.values)\n",
        "elif type(max_mdl).__name__ == 'SVC':\n",
        "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
        "  shap_values = explainer(X_test.values)\n",
        "elif type(max_mdl).__name__ == 'NuSVC':\n",
        "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
        "  shap_values = explainer(X_test.values)\n",
        "\n",
        "# Get the Shapley values for the X_test data...\n",
        "#shap_values = explainer.shap_values(X_test.values)"
      ],
      "metadata": {
        "id": "_xCoD7kw6r0b"
      },
      "id": "_xCoD7kw6r0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Analyst and technical explainability"
      ],
      "metadata": {
        "id": "bdMhCBHMFf4v"
      },
      "id": "bdMhCBHMFf4v"
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyst KPI...\n",
        "kpi_review_analyst(mdl=max_mdl,X=X_test, y=y_test, y_hat=y_test_hat)"
      ],
      "metadata": {
        "id": "fPlSWba2dSoo"
      },
      "id": "fPlSWba2dSoo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "fQA6gVOre56h"
      },
      "id": "fQA6gVOre56h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature importance"
      ],
      "metadata": {
        "id": "8u7btmtyFKNn"
      },
      "id": "8u7btmtyFKNn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the feature importance\n",
        "shap.plots.bar(shap_values, max_display=30)"
      ],
      "metadata": {
        "id": "NfwlscWzExPQ"
      },
      "id": "NfwlscWzExPQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Compliance and Regulatory\n",
        "\n"
      ],
      "metadata": {
        "id": "ce1fcWInh48Q"
      },
      "id": "ce1fcWInh48Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "See if any features are suspiciously important for one gender vs another."
      ],
      "metadata": {
        "id": "fxV86fMFKX7r"
      },
      "id": "fxV86fMFKX7r"
    },
    {
      "cell_type": "code",
      "source": [
        "X_test['gender']"
      ],
      "metadata": {
        "id": "geMC5WAshfuT"
      },
      "id": "geMC5WAshfuT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter importance by gender..\n",
        "gdr = [\"male\" if x==1 else \"female\" for x in X_test['gender']]\n",
        "\n",
        "# Plot the feature importance\n",
        "shap.plots.bar(shap_values.cohorts(gdr).abs.mean(0))"
      ],
      "metadata": {
        "id": "nqJGtpmu1nig"
      },
      "id": "nqJGtpmu1nig",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
        "Madhu\n",
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
        "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ "
      ],
      "metadata": {
        "id": "5MGWlux3KvFy"
      },
      "id": "5MGWlux3KvFy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Customer\n",
        "\n",
        "A customer may want to knowlt he sensitivity of the decision to certain characteristics. IF a refusal has been made, it would be beneficial to report to the customer what they need to change to get a favorable outcome. \n",
        "\n",
        "We can use the SHAP waterfall plot to help"
      ],
      "metadata": {
        "id": "De8c3JUdKvtR"
      },
      "id": "De8c3JUdKvtR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Find a customer with bad credit prediction\n",
        "for rejected_eg_rowno in range(0,y_test.shape[0]):\n",
        "  if y_test.iloc[rejected_eg_rowno] == 1:\n",
        "    break\n",
        "\n",
        "# This customer was refused credit and we can provide an explanation for their refusal...\n",
        "shap.plots.waterfall(shap_values[rejected_eg_rowno])"
      ],
      "metadata": {
        "id": "4ZHyTdYTLCh8"
      },
      "id": "4ZHyTdYTLCh8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.reset_index()"
      ],
      "metadata": {
        "id": "dha9E2RRLRvI"
      },
      "id": "dha9E2RRLRvI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 5: Model Monitoring and Reporting\n",
        "\n",
        "Our model is now in production and being used in practice. We are \"risk on\" and we need to continually monitor and record our KPIs. Individuals accountability for these processes is critical.\n",
        "\n",
        "We also need to monitor data drift. If the distribution or the nature of the data we pass into our model substantioally differs from our training data, our model results will almost certainly be garbage. We need to monitor data drift and should drift occurr we need to re-run our model developmnent process to traing an appropriate model. This would take us back to Stage2 in this process.\n",
        "\n",
        "There is also the question of whether our model is still the best approach as time steps forward? Our model paramaters may become stale, the model itself may be less appropriaytr given its inductive biases. Many things can change and to monitor this we should consider using a challenger model. Challenger models are used to compete against our live model, and we should monitor our KPIs generated by our live model and compare them to those produced by the challenger model. "
      ],
      "metadata": {
        "id": "TTN8r_7zRDbt"
      },
      "id": "TTN8r_7zRDbt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage5a: Data Drift"
      ],
      "metadata": {
        "id": "j3se-Ud-Spu3"
      },
      "id": "j3se-Ud-Spu3"
    },
    {
      "cell_type": "code",
      "source": [
        "# AUGUSTINE! Add your function call HERE"
      ],
      "metadata": {
        "id": "QREnQ1poSmRR"
      },
      "id": "QREnQ1poSmRR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage5b: Challenger Models\n",
        "\n",
        "So what model do we use as a challenger, and when do we traing it? There are no perfect answers but we can use our model development pipeline to help. Note that our model selection function, auto_classifier_selection, allowed us to generate a range of classifiers, we can use one or all of these classifiers as our challenger. If we decided to ensemble the classifiers as our live model (ir combine the results of all in a voting ensemble for instance) we can select the best single model as our challeger. If we decided to use the best single model as our live model, we can use a boting ensemble as our challenger.\n",
        "\n"
      ],
      "metadata": {
        "id": "4wmYyfyWRmrR"
      },
      "id": "4wmYyfyWRmrR"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run the challenger ensemble\n",
        "y_test_hat, ens_mdl = challenger_ensemble_run(all_mdls=all_models,\n",
        "                        all_mdls_desc=all_models_desc,\n",
        "                        all_mdls_F1score=all_mdls_F1score,\n",
        "                        X_train=X_train\n",
        "                        y_train=y_train\n",
        "                        X_test=X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "_8v9zlaWTd3F"
      },
      "id": "_8v9zlaWTd3F",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}